%\VignetteIndexEntry{solutions3}
%\VignetteEngine{Sweave}


\documentclass[a4paper,justified,openany]{tufte-handout}\usepackage{knitr}

\usepackage{amsmath}
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}
\title{Predictive Analytics: practical 3}
\date{} % if the \date{} command is left out, the current date will be used
\usepackage{booktabs}
\usepackage{units}
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}
\newcommand{\cc}{\texttt}
\graphicspath{{../graphics/}}
\setcounter{secnumdepth}{2}
\usepackage{microtype}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\maketitle% this prints the handout title, author, and date

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(}\hlstr{"caret"}\hlstd{)}
\hlkwd{data}\hlstd{(FuelEconomy,} \hlkwc{package} \hlstd{=} \hlstr{"AppliedPredictiveModeling"}\hlstd{)}
\hlkwd{set.seed}\hlstd{(}\hlnum{1}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\section*{Resampling methods}

\begin{itemize}
  \item Fit a KNN regression model to the \cc{cars2010} data set with \cc{FE} as the response.\marginnote{The data set can be loaded \cc{data("FuelEconomy", package = "AppliedPredictiveModeling")}.}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mKNN} \hlkwb{=} \hlkwd{train}\hlstd{(FE} \hlopt{~} \hlstd{.,} \hlkwc{method} \hlstd{=} \hlstr{"knn"}\hlstd{,} \hlkwc{data} \hlstd{= cars2010)}
\end{alltt}
\end{kframe}
\end{knitrout}

  \item Estimate test error using the validation set approach explored at the beginning of the chapter
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# create a random sample to hold out}
\hlstd{i} \hlkwb{=} \hlkwd{sample}\hlstd{(}\hlkwd{nrow}\hlstd{(cars2010),}\hlnum{100}\hlstd{)}
\hlcom{# set the train control object}
\hlstd{tc} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{1}\hlstd{,}
    \hlkwc{index} \hlstd{=} \hlkwd{list}\hlstd{(}\hlkwc{Fold1} \hlstd{= (}\hlnum{1}\hlopt{:}\hlkwd{nrow}\hlstd{(cars2010))[}\hlopt{-}\hlstd{i]))}
\hlcom{# fit the model using this train control object}
\hlstd{mKNNvs} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{.,} \hlkwc{method} \hlstd{=} \hlstr{"knn"}\hlstd{,} \hlkwc{data} \hlstd{= cars2010,}
    \hlkwc{trControl} \hlstd{= tc)}
\end{alltt}
\end{kframe}
\end{knitrout}


  \item Using the same validation set, estimate the performance of the k nearest neighbours algorithm for different values of $k$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mKNNvs2} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{.,} \hlkwc{method} \hlstd{=} \hlstr{"knn"}\hlstd{,} \hlkwc{data} \hlstd{= cars2010,}
     \hlkwc{trControl} \hlstd{= tc,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{k}\hlstd{=} \hlnum{2}\hlopt{:}\hlnum{20}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}
  \item Which model is chosen as the best when using the validation set approach?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## With set.seed(1)}
\hlstd{mKNNvs2}\hlopt{$}\hlstd{bestTune}
\end{alltt}
\begin{verbatim}
##   k
## 2 3
\end{verbatim}
\end{kframe}
\end{knitrout}

\item Create new \cc{trainControl} objects to specify the use of 5 fold and 10 fold cross validation as well as bootstrapping to estimate test MSE.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{tc5fold} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{5}\hlstd{)}
\hlstd{tc10fold} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{10}\hlstd{)}
\hlcom{# use 50 boot strap estimates}
\hlstd{tcboot} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"boot"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{50}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

  \item Go through the same training procedure attempting to find the best KNN model.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mKNNcv5} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{.,} \hlkwc{data} \hlstd{= cars2010,} \hlkwc{method} \hlstd{=} \hlstr{"knn"}\hlstd{,}
    \hlkwc{trControl} \hlstd{= tc5fold,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{k} \hlstd{=} \hlnum{2}\hlopt{:}\hlnum{20}\hlstd{))}

\hlstd{mKNNcv10} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{.,} \hlkwc{data} \hlstd{= cars2010,} \hlkwc{method} \hlstd{=} \hlstr{"knn"}\hlstd{,}
    \hlkwc{trControl} \hlstd{= tc10fold,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{k} \hlstd{=} \hlnum{2}\hlopt{:}\hlnum{20}\hlstd{))}

\hlstd{mKNNboot} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{.,} \hlkwc{data} \hlstd{= cars2010,} \hlkwc{method} \hlstd{=} \hlstr{"knn"}\hlstd{,}
    \hlkwc{trControl} \hlstd{= tcboot,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{k} \hlstd{=} \hlnum{2}\hlopt{:}\hlnum{20}\hlstd{))}
\hlstd{mKNNcv5}\hlopt{$}\hlstd{bestTune}
\end{alltt}
\begin{verbatim}
##   k
## 1 2
\end{verbatim}
\begin{alltt}
\hlstd{mKNNcv10}\hlopt{$}\hlstd{bestTune}
\end{alltt}
\begin{verbatim}
##   k
## 1 2
\end{verbatim}
\begin{alltt}
\hlstd{mKNNboot}\hlopt{$}\hlstd{bestTune}
\end{alltt}
\begin{verbatim}
##   k
## 1 2
\end{verbatim}
\end{kframe}
\end{knitrout}

  \item How do the results vary based on the method of estimation?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#The k-fold cross validation estimates and bootstrap estimates all }
\hlcom{#yield the same conclusion, however it is different to when we used }
\hlcom{#validation set approach earlier. We could plot the results }
\hlcom{# from each on one plot to compare further:}
\hlkwd{plot}\hlstd{(}\hlnum{2}\hlopt{:}\hlnum{20}\hlstd{, mKNNboot}\hlopt{$}\hlstd{results[,}\hlnum{2}\hlstd{],}\hlkwc{type} \hlstd{=} \hlstr{"l"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"RMSE"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"k"}\hlstd{,} \hlkwc{ylim} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{3}\hlstd{,}\hlnum{6.5}\hlstd{))}
\hlkwd{lines}\hlstd{(}\hlnum{2}\hlopt{:}\hlnum{20}\hlstd{, mKNNcv10}\hlopt{$}\hlstd{results[,}\hlnum{2}\hlstd{],} \hlkwc{col} \hlstd{=} \hlstr{"red"}\hlstd{)}
\hlkwd{lines}\hlstd{(}\hlnum{2}\hlopt{:}\hlnum{20}\hlstd{, mKNNcv5}\hlopt{$}\hlstd{results[,}\hlnum{2}\hlstd{],} \hlkwc{col} \hlstd{=} \hlstr{"blue"}\hlstd{)}
\hlkwd{lines}\hlstd{(}\hlnum{2}\hlopt{:}\hlnum{20}\hlstd{, mKNNvs2}\hlopt{$}\hlstd{results[,}\hlnum{2}\hlstd{],} \hlkwc{col} \hlstd{=} \hlstr{"green"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
  \item Are the conclusions always the same?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#no see previous answer}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{itemize}

\noindent If we add the \cc{returnResamp = "all"} argument in the trainControl function we can plot the resampling distributions, see figure~\ref{fig:cvresamp}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{tc} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{15}\hlstd{,}
                  \hlkwc{returnResamp} \hlstd{=} \hlstr{"all"}\hlstd{)}
\hlstd{m} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{.,} \hlkwc{data} \hlstd{= cars2010,} \hlkwc{method} \hlstd{=} \hlstr{"knn"}\hlstd{,}
          \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{k} \hlstd{=} \hlnum{1}\hlopt{:}\hlnum{15}\hlstd{),} \hlkwc{trControl} \hlstd{= tc)}

\hlkwd{boxplot}\hlstd{(RMSE}\hlopt{~}\hlstd{k,} \hlkwc{data} \hlstd{= m}\hlopt{$}\hlstd{resample)}
\end{alltt}
\end{kframe}
\end{knitrout}


\begin{figure}[t]
  \centering
  \includegraphics[width = \textwidth]{graphics/cvresamp-crop}
  \caption{$15$ fold cross validation estimates of RMSE in a $K$ nearest neighbours model against number of nearest neighbours.}
  \label{fig:cvresamp}
\end{figure}


We can overlay the information from each method using \cc{add = TRUE}. In addition we could compare the computational cost of each of the methods. The output list from a \cc{train} object contains timing information which can be accessed
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{m}\hlopt{$}\hlstd{time}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{itemize}
  \item Which method is the most computationally efficient?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mKNNvs2}\hlopt{$}\hlstd{time}\hlopt{$}\hlstd{everything}
\end{alltt}
\begin{verbatim}
##    user  system elapsed 
##   0.412   0.000   0.414
\end{verbatim}
\begin{alltt}
\hlstd{mKNNcv5}\hlopt{$}\hlstd{time}\hlopt{$}\hlstd{everything}
\end{alltt}
\begin{verbatim}
##    user  system elapsed 
##   1.484   0.000   1.490
\end{verbatim}
\begin{alltt}
\hlstd{mKNNcv10}\hlopt{$}\hlstd{time}\hlopt{$}\hlstd{everything}
\end{alltt}
\begin{verbatim}
##    user  system elapsed 
##   1.784   0.000   1.790
\end{verbatim}
\begin{alltt}
\hlstd{mKNNboot}\hlopt{$}\hlstd{time}\hlopt{$}\hlstd{everything}
\end{alltt}
\begin{verbatim}
##    user  system elapsed 
##   25.57    0.00   25.65
\end{verbatim}
\begin{alltt}
\hlcom{# The validation set approach was quickest, however we must bear in mind}
\hlcom{# that the conclusion here was different to the other cross validation}
\hlcom{# approaches. The two k--fold cross validation estimates of RMSE and the}
\hlcom{# bootstrap estimates all agreed with each other lending more weight to}
\hlcom{# their conclusions. Plus we saw in the lectures that validation set}
\hlcom{# approach was prone to highly variable estimates meaning we could get a}
\hlcom{# different conclusion using a different hold out set. Either of the two}
\hlcom{# k--fold cross validation methods would be preferable here.}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{itemize}

\section*{Penalised regression}

The \cc{diabetes} data set in the \cc{lars} package contains measurements of a number of predictors to model a response $y$, a measure of disease progression. There are other columns in the data set which contain interactions so we will extract just the predictors and the response. The data has already been normalized.

% <<>>=
% data(diabetes, package = "lars")
% 
% diabetesdata = cbind(diabetes$x,"y" = diabetes$y)
% # shortcut to create a model formula with all 2 way 
% # interactions and square terms.
% modelFormula = as.formula(paste("y~(.)^2 + ",
%   paste("I(",colnames(diabetesdata[,1:10]),"^2)", 
%         collapse = "+",sep = "")))
% 
% @

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(diabetes,} \hlkwc{package} \hlstd{=} \hlstr{"lars"}\hlstd{)}
\hlstd{diabetesdata} \hlkwb{=} \hlkwd{cbind}\hlstd{(diabetes}\hlopt{$}\hlstd{x,}\hlstr{"y"} \hlstd{= diabetes}\hlopt{$}\hlstd{y)}
\end{alltt}
\end{kframe}
\end{knitrout}


\begin{itemize}
\item Try fitting a lasso, ridge and elastic net model using all of the main effects, pairwise interactions and square terms from each of the predictors.\sidenote{Hint: see notes for shortcut on creating model formula. Also be aware that if the predictor is a factor a polynomial term doesn't make sense}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## load the data in }
\hlstd{modelformula} \hlkwb{=} \hlkwd{as.formula}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlstr{"y~(.)^2 + "}\hlstd{,}
    \hlkwd{paste0}\hlstd{(}\hlstr{"I("}\hlstd{,}\hlkwd{colnames}\hlstd{(diabetesdata),}\hlstr{"^2)"}\hlstd{,}
          \hlkwc{collapse} \hlstd{=} \hlstr{"+"}\hlstd{)}
    \hlstd{))}
\hlstd{mLASSO} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"lasso"}\hlstd{)}
\hlstd{mRIDGE} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"ridge"}\hlstd{)}
\hlstd{mENET} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"enet"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
\marginnote{fraction = 0 is the same as the null model.}
\marginnote{ 
\cc{$y \sim (.) \wedge 2$}
is short hand for a model that includes pairwise interactions for each predictor, so if we use this we should only need to add the square terms}

% <<warning = FALSE, message = FALSE, echo = FALSE>>=
% m.lasso = train(modelFormula, data = diabetesdata, 
%         method = "lasso", 
%         tuneGrid = data.frame(fraction = seq(0,1,0.05)))
% @
% 
% <<warning = FALSE, message = FALSE, echo = FALSE>>=
% m.lasso = train(modelFormula, data = diabetesdata, 
%         method = "lasso", 
%         tuneGrid = data.frame(fraction = seq(0,1,0.05)))
% @


  \item Try to narrow in on the region of lowest RMSE for each model, don't forget about the \cc{tuneGrid} argument to the train function.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# examine previous output then train over a finer grid near the better end}
\hlstd{mLASSOfine} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,}\hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"lasso"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{fraction} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{0.1}\hlstd{,}\hlnum{0.5}\hlstd{,}\hlkwc{by} \hlstd{=} \hlnum{0.05}\hlstd{)))}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning: model fit failed for Resample01: fraction=0.5 Error in if (zmin < gamhat) \{ : missing value where TRUE/FALSE needed}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : There were missing values in resampled performance measures.}}\begin{alltt}
\hlstd{mLASSOfine}\hlopt{$}\hlstd{results}
\end{alltt}
\begin{verbatim}
##   fraction  RMSE Rsquared RMSESD RsquaredSD
## 1     0.10 17.24   0.9496  1.045   0.006322
## 2     0.15 17.62   0.9474  1.010   0.006045
## 3     0.20 17.84   0.9461  1.084   0.006533
## 4     0.25 17.97   0.9454  1.154   0.006872
## 5     0.30 18.06   0.9448  1.217   0.007202
## 6     0.35 18.12   0.9445  1.285   0.007623
## 7     0.40 18.16   0.9443  1.342   0.007992
## 8     0.45 18.19   0.9441  1.403   0.008400
## 9     0.50 18.23   0.9438  1.464   0.008831
\end{verbatim}
\begin{alltt}
\hlcom{# best still right down at the 0.1 end}
\hlstd{mLASSOfiner} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,}\hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"lasso"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{fraction} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{0.01}\hlstd{,}\hlnum{0.15}\hlstd{,}\hlkwc{by} \hlstd{=} \hlnum{0.01}\hlstd{)))}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning: model fit failed for Resample24: fraction=0.15 Error in if (zmin < gamhat) \{ : missing value where TRUE/FALSE needed}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : There were missing values in resampled performance measures.}}\begin{alltt}
\hlstd{mLASSOfiner}\hlopt{$}\hlstd{results}
\end{alltt}
\begin{verbatim}
##    fraction  RMSE Rsquared RMSESD RsquaredSD
## 1      0.01 48.82   0.9543 16.879   0.002663
## 2      0.02 32.54   0.9543 17.893   0.003012
## 3      0.03 26.27   0.9548 15.299   0.003783
## 4      0.04 23.11   0.9540 12.375   0.004271
## 5      0.05 21.16   0.9533  9.923   0.005061
## 6      0.06 19.90   0.9530  7.726   0.005499
## 7      0.07 19.04   0.9523  5.709   0.005537
## 8      0.08 18.33   0.9516  4.045   0.005596
## 9      0.09 17.85   0.9513  2.701   0.006130
## 10     0.10 17.56   0.9508  1.682   0.006680
## 11     0.11 17.39   0.9502  1.289   0.006990
## 12     0.12 17.39   0.9496  1.312   0.007265
## 13     0.13 17.47   0.9491  1.378   0.007689
## 14     0.14 17.56   0.9486  1.426   0.008035
## 15     0.15 17.65   0.9480  1.467   0.008370
\end{verbatim}
\begin{alltt}
\hlcom{# 0.09 seems the best}

\hlstd{mRIDGEfine} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,}\hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"ridge"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{lambda} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{0.1}\hlstd{,}\hlkwc{by} \hlstd{=} \hlnum{0.01}\hlstd{)))}
\hlstd{mRIDGEfine}\hlopt{$}\hlstd{results}
\end{alltt}
\begin{verbatim}
##    lambda  RMSE Rsquared RMSESD RsquaredSD
## 1    0.00 18.03   0.9459 1.0195   0.007486
## 2    0.01 16.90   0.9522 0.8943   0.005874
## 3    0.02 16.83   0.9525 0.8872   0.005968
## 4    0.03 16.87   0.9523 0.9050   0.006233
## 5    0.04 16.96   0.9517 0.9359   0.006585
## 6    0.05 17.10   0.9509 0.9737   0.006985
## 7    0.06 17.28   0.9498 1.0148   0.007411
## 8    0.07 17.48   0.9487 1.0574   0.007853
## 9    0.08 17.70   0.9474 1.1002   0.008303
## 10   0.09 17.94   0.9460 1.1426   0.008758
## 11   0.10 18.19   0.9445 1.1843   0.009214
\end{verbatim}
\begin{alltt}
\hlstd{mRIDGEfiner} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,}\hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"ridge"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{lambda} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{0.005}\hlstd{,}\hlnum{0.03}\hlstd{,}\hlkwc{by} \hlstd{=} \hlnum{0.001}\hlstd{)))}
\hlstd{mRIDGEfiner}\hlopt{$}\hlstd{results}
\end{alltt}
\begin{verbatim}
##    lambda  RMSE Rsquared RMSESD RsquaredSD
## 1   0.005 16.69   0.9525 0.7355   0.003735
## 2   0.006 16.66   0.9526 0.7287   0.003736
## 3   0.007 16.63   0.9528 0.7233   0.003747
## 4   0.008 16.61   0.9529 0.7189   0.003764
## 5   0.009 16.59   0.9530 0.7153   0.003787
## 6   0.010 16.58   0.9530 0.7124   0.003813
## 7   0.011 16.57   0.9531 0.7102   0.003841
## 8   0.012 16.56   0.9531 0.7085   0.003872
## 9   0.013 16.55   0.9532 0.7072   0.003904
## 10  0.014 16.54   0.9532 0.7063   0.003938
## 11  0.015 16.54   0.9532 0.7057   0.003972
## 12  0.016 16.54   0.9532 0.7055   0.004008
## 13  0.017 16.54   0.9532 0.7055   0.004044
## 14  0.018 16.54   0.9532 0.7058   0.004081
## 15  0.019 16.54   0.9532 0.7063   0.004118
## 16  0.020 16.54   0.9532 0.7070   0.004156
## 17  0.021 16.54   0.9532 0.7079   0.004195
## 18  0.022 16.55   0.9531 0.7090   0.004233
## 19  0.023 16.55   0.9531 0.7102   0.004272
## 20  0.024 16.56   0.9531 0.7115   0.004311
## 21  0.025 16.56   0.9530 0.7130   0.004351
## 22  0.026 16.57   0.9530 0.7146   0.004390
## 23  0.027 16.58   0.9529 0.7163   0.004430
## 24  0.028 16.59   0.9529 0.7182   0.004469
## 25  0.029 16.59   0.9528 0.7201   0.004509
## 26  0.030 16.60   0.9528 0.7221   0.004549
\end{verbatim}
\begin{alltt}
\hlcom{# 0.023 seems best}

\hlstd{mENETfine} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"enet"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{expand.grid}\hlstd{(}
                         \hlkwc{lambda} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0.001}\hlstd{,}\hlnum{0.01}\hlstd{,}\hlnum{0.1}\hlstd{),}
                         \hlkwc{fraction} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0.4}\hlstd{,}\hlnum{0.5}\hlstd{,}\hlnum{0.6}\hlstd{)}
    \hlstd{))}
\hlstd{mENETfine}\hlopt{$}\hlstd{results}
\end{alltt}
\begin{verbatim}
##   lambda fraction  RMSE Rsquared RMSESD RsquaredSD
## 1  0.001      0.4 16.37   0.9568 0.7865   0.003754
## 4  0.010      0.4 16.53   0.9579 1.1314   0.003818
## 7  0.100      0.4 22.85   0.9553 2.5052   0.003438
## 2  0.001      0.5 16.76   0.9547 0.8179   0.004496
## 5  0.010      0.5 15.93   0.9590 0.7400   0.003178
## 8  0.100      0.5 17.05   0.9566 1.1056   0.004016
## 3  0.001      0.6 16.95   0.9537 0.8400   0.004805
## 6  0.010      0.6 16.28   0.9573 0.7709   0.003461
## 9  0.100      0.6 16.72   0.9547 0.8511   0.004989
\end{verbatim}
\begin{alltt}
\hlstd{mENETfiner} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"enet"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{expand.grid}\hlstd{(}
                         \hlkwc{lambda} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{0.001}\hlstd{,}\hlnum{0.1}\hlstd{,}\hlkwc{length.out} \hlstd{=} \hlnum{10}\hlstd{),}
                         \hlkwc{fraction} \hlstd{=} \hlnum{0.5}\hlstd{))}
\hlstd{mENETfiner}\hlopt{$}\hlstd{results}
\end{alltt}
\begin{verbatim}
##    lambda fraction  RMSE Rsquared RMSESD RsquaredSD
## 1   0.001      0.5 16.76   0.9539 0.8983   0.004779
## 2   0.012      0.5 15.94   0.9581 0.6089   0.003401
## 3   0.023      0.5 15.96   0.9583 0.6562   0.003541
## 4   0.034      0.5 16.16   0.9579 0.6988   0.003580
## 5   0.045      0.5 16.38   0.9574 0.7513   0.003529
## 6   0.056      0.5 16.57   0.9570 0.8163   0.003579
## 7   0.067      0.5 16.71   0.9567 0.8622   0.003605
## 8   0.078      0.5 16.82   0.9565 0.8949   0.003685
## 9   0.089      0.5 16.92   0.9562 0.9163   0.003809
## 10  0.100      0.5 16.99   0.9559 0.9337   0.003945
\end{verbatim}
\begin{alltt}
\hlcom{# 0.012, 0.5 best}
\end{alltt}
\end{kframe}
\end{knitrout}
We can view what the coefficients will be by using
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{coef} \hlkwb{=} \hlkwd{predict}\hlstd{(m.lasso}\hlopt{$}\hlstd{finalModel,}
        \hlkwc{mode} \hlstd{=} \hlstr{"fraction"}\hlstd{,}
        \hlkwc{s} \hlstd{=} \hlnum{0.1}\hlstd{,}\hlcom{# which ever fraction was chosen as best}
        \hlkwc{type} \hlstd{=} \hlstr{"coefficients"}
\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}


  \item How many features have been chosen by the lasso and enet models?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# use predict to find the coefficients}
\hlstd{coefLASSO} \hlkwb{=} \hlkwd{predict}\hlstd{(mLASSOfiner}\hlopt{$}\hlstd{finalModel,} \hlkwc{mode} \hlstd{=} \hlstr{"fraction"}\hlstd{,}
        \hlkwc{type} \hlstd{=} \hlstr{"coefficient"}\hlstd{,} \hlkwc{s} \hlstd{=} \hlnum{0.09}
        \hlstd{)}
\hlkwd{sum}\hlstd{(coefLASSO}\hlopt{$}\hlstd{coefficients} \hlopt{!=} \hlnum{0}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 54
\end{verbatim}
\begin{alltt}
\hlstd{coefENET}\hlkwb{=} \hlkwd{predict}\hlstd{(mENETfiner}\hlopt{$}\hlstd{finalModel,} \hlkwc{mode} \hlstd{=} \hlstr{"fraction"}\hlstd{,}
        \hlkwc{type} \hlstd{=} \hlstr{"coefficient"}\hlstd{,} \hlkwc{s} \hlstd{=} \hlnum{0.5}
        \hlstd{)}
\hlkwd{sum}\hlstd{(coefENET}\hlopt{$}\hlstd{coefficients} \hlopt{!=} \hlnum{0}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 21
\end{verbatim}
\end{kframe}
\end{knitrout}
  \item How do these models compare to principal components and partial least squares regression?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mPCR} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,} \hlkwc{method} \hlstd{=} \hlstr{"pcr"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{ncomp} \hlstd{=} \hlnum{1}\hlopt{:}\hlnum{7}\hlstd{))}
\hlstd{mPLS} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,} \hlkwc{method} \hlstd{=} \hlstr{"pls"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{ncomp} \hlstd{=} \hlnum{1}\hlopt{:}\hlnum{7}\hlstd{))}
\hlstd{mPLS2} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,} \hlkwc{method} \hlstd{=} \hlstr{"pls"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{ncomp} \hlstd{=} \hlnum{5}\hlopt{:}\hlnum{15}\hlstd{))}
\hlkwd{getTrainPerf}\hlstd{(mLASSOfiner)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared method
## 1     17.39        0.9496  lasso
\end{verbatim}
\begin{alltt}
\hlkwd{getTrainPerf}\hlstd{(mRIDGEfiner)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared method
## 1     16.54        0.9532  ridge
\end{verbatim}
\begin{alltt}
\hlkwd{getTrainPerf}\hlstd{(mENETfiner)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared method
## 1     15.94        0.9581   enet
\end{verbatim}
\begin{alltt}
\hlkwd{getTrainPerf}\hlstd{(mPCR)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared method
## 1     16.55        0.9557    pcr
\end{verbatim}
\begin{alltt}
\hlkwd{getTrainPerf}\hlstd{(mPLS2)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared method
## 1     15.79        0.9585    pls
\end{verbatim}
\begin{alltt}
\hlcom{# The elastic net model has the lowest estimated test error, all are fairly}
\hlcom{# similar. The elastic net model suggests only 21 non--zero coefficients out}
\hlcom{# of all of those included in the model.}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{itemize}



\section*{Advanced}

\marginnote{This section is intended for users who have a more in depth background to R programming. Attendance to the Programming in R course should be adequate background.}

So far we have only used default functions and metrics to compare the performance of models, however we are not restricted to doing this. For example, training of classification models is typically more difficult when there is an imbalance in the two classes in the training set. Models trained from such data typically have high specificity but poor sensitivity or vice versa. Instead of training to maximise accuracy using data from the training set we could try to maximise according to some other criteria, namely sensitivity and specificity being as close to perfect as possible $(1, 1)$.

To add our function we need to make sure we mirror the structure of those included in caret already.\marginnote{We can view a functions code by typing its name with no brackets.} The following code creates a new function that could be used to summarise a model
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fourStats} \hlkwb{=} \hlkwa{function} \hlstd{(}\hlkwc{data}\hlstd{,} \hlkwc{lev} \hlstd{=} \hlkwa{NULL}\hlstd{,} \hlkwc{model} \hlstd{=} \hlkwa{NULL}\hlstd{) \{}
  \hlcom{# This code will use the area under the ROC curve and the}
  \hlcom{# sensitivity and specificity values from the built in }
  \hlcom{# twoClassSummary function}
  \hlstd{out} \hlkwb{=} \hlkwd{twoClassSummary}\hlstd{(data,} \hlkwc{lev} \hlstd{=} \hlkwd{levels}\hlstd{(data}\hlopt{$}\hlstd{obs),}
                        \hlkwc{model} \hlstd{=} \hlkwa{NULL}\hlstd{)}
  \hlcom{# The best possible model has sensitivity of 1 and }
  \hlcom{# specifity of 1. How far are we from that value?}
  \hlstd{coords} \hlkwb{=} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{1}\hlstd{, out[}\hlstr{"Spec"}\hlstd{], out[}\hlstr{"Sens"}\hlstd{]),}
                   \hlkwc{ncol} \hlstd{=} \hlnum{2}\hlstd{,}
                   \hlkwc{byrow} \hlstd{=} \hlnum{TRUE}\hlstd{)}
  \hlcom{# return the disctance measure together with the }
  \hlcom{# output from two class summary}
  \hlkwd{c}\hlstd{(}\hlkwc{Dist} \hlstd{=} \hlkwd{dist}\hlstd{(coords)[}\hlnum{1}\hlstd{], out)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
\noindent we could then use this in the \cc{train} function
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(Sonar,} \hlkwc{package} \hlstd{=} \hlstr{"mlbench"}\hlstd{)}
\hlstd{mod} \hlkwb{=} \hlkwd{train}\hlstd{(Class} \hlopt{~} \hlstd{.,} \hlkwc{data} \hlstd{= Sonar,}
              \hlkwc{method} \hlstd{=} \hlstr{"knn"}\hlstd{,}
              \hlcom{# Minimize the distance to the perfect model}
              \hlkwc{metric} \hlstd{=} \hlstr{"Dist"}\hlstd{,}
              \hlkwc{maximize} \hlstd{=} \hlnum{FALSE}\hlstd{,}
              \hlkwc{tuneLength} \hlstd{=} \hlnum{20}\hlstd{,}
              \hlkwc{trControl} \hlstd{=}
    \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{classProbs} \hlstd{=} \hlnum{TRUE}\hlstd{,}
                     \hlkwc{summaryFunction} \hlstd{= fourStats))}
\end{alltt}
\end{kframe}
\end{knitrout}

\noindent The \cc{plot} function will then show the profile of the resampling estimates of our chosen statistic against the tuning parameters, see figure~\ref{fig:newsummary}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(mod)}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{marginfigure}
  \includegraphics[]{graphics/newsummary-crop}
  \caption{Plot of the distance from a perfect classifier measured by sensitivity and specificity against tuning parameter for a k nearest neighbour model.}
  \label{fig:newsummary}
\end{marginfigure}

\begin{itemize}
  \item Have a go at writing a function that will allow a regression model to be chosen by the absolute value of the largest residual and try using it to fit a couple of models.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{maxabsres} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{data}\hlstd{,} \hlkwc{lev} \hlstd{=} \hlkwa{NULL}\hlstd{,} \hlkwc{model} \hlstd{=} \hlkwa{NULL}\hlstd{) \{}
    \hlstd{m} \hlkwb{=} \hlkwd{max}\hlstd{(}\hlkwd{abs}\hlstd{(data}\hlopt{$}\hlstd{obs} \hlopt{-} \hlstd{data}\hlopt{$}\hlstd{pred))}
    \hlkwd{return}\hlstd{(}\hlkwd{c}\hlstd{(}\hlkwc{Max} \hlstd{= m))}
\hlstd{\}}
\hlcom{# test with pls regression}
\hlstd{tccustom} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{summaryFunction} \hlstd{= maxabsres)}
\hlstd{mPLScustom} \hlkwb{=} \hlkwd{train}\hlstd{(FE} \hlopt{~} \hlstd{.,} \hlkwc{data} \hlstd{= cars2010,} \hlkwc{method} \hlstd{=} \hlstr{"pls"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{ncomp} \hlstd{=} \hlnum{1}\hlopt{:}\hlnum{6}\hlstd{),}
    \hlkwc{trControl} \hlstd{= tccustom,} \hlkwc{metric} \hlstd{=} \hlstr{"Max"}\hlstd{,} \hlkwc{maximize} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlcom{# success not to sugges this is a good choice of metric}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{itemize}
% \noindent Have a go at writing a function that will allow a regression models by the absolute value of the largest residual and try using it to fit a couple of models.

\end{document}


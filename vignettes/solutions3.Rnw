%\VignetteIndexEntry{solutions3}
%!Snw weave = knitr
%\VignetteEngine{knitr::knitr}


\documentclass[a4paper,justified,openany]{tufte-handout}
<<setup, echo=FALSE, cache=FALSE>>=

library(knitr)
opts_knit$set(self.contained=FALSE, tidy = TRUE, 
              cache = TRUE, size = "small", message = FALSE,
              fig.path='knitr_figure/graphics-', 
               cache.path='knitr_cache/graphics-', 
               fig.align='center', 
               dev='pdf', fig.width=5, fig.height=5)

knit_hooks$set(par=function(before, options, envir){
    if (before && options$fig.show!='none') {
        par(mar=c(3,3,2,1),cex.lab=.95,cex.axis=.9,
            mgp=c(2,.7,0),tcl=-.01, las=1)
}}, crop=hook_pdfcrop)
# knit_theme$set(knit_theme$get("greyscale0"))

opts_knit$set(out.format = "latex")

# options(replace.assign=FALSE,width=50)
# opts_chunk$set(fig.path='knitr_figure/graphics-',
# cache.path='knitr_cache/graphics-',
# fig.align='center',
# dev='pdf', fig.width=5, fig.height=5,
# fig.show='hold', cache=FALSE, par=TRUE)
# knit_hooks$set(crop=hook_pdfcrop)
# knit_hooks$set(par=function(before, options, envir){
# if (before && options$fig.show!='none') {
# par(mar=c(3,3,2,1),cex.lab=.95,cex.axis=.9,
# mgp=c(2,.7,0),tcl=-.01, las=1)
# }}, crop=hook_pdfcrop)

if(!file.exists("graphics")) dir.create("graphics")
@
\usepackage{amsmath}
% Set up the images/graphics package
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
%\graphicspath{{vignettes/graphics/}}
\title{Predictive Analytics: solutions 3}
\date{} % if the \date{} command is left out, the current date will be used
% The following package makes prettier tables. We're all about the bling!
\usepackage{booktabs}
% The units package provides nice, non-stacked fractions and better spacing
% for units.
\usepackage{units}
% The fancyvrb package lets us customize the formatting of verbatim
% environments. We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}
\newcommand{\cc}{\texttt}
\graphicspath{{../graphics/}}
\setcounter{secnumdepth}{2}
\usepackage{microtype}
\begin{document}
\maketitle% this prints the handout title, author, and date

Load the packages and data
<<>>=
library(nclRpredictive)
library(caret)
data(FuelEconomy, package = "AppliedPredictiveModeling")
set.seed(1)
@

\begin{itemize}
\item Fit a KNN regression model to the \cc{cars2010} data set with \cc{FE} as the response.
<<>>=
mKNN = train(FE~., method = "knn", data = cars2010)
@ 
  \item Estimate test error using the validation set approach explored at the beginning of the chapter
<<>>=
# create a random sample to hold out
i = sample(nrow(cars2010),100)
# set the train control object
tc = trainControl(method = "cv", number = 1,
    index = list(Fold1 = (1:nrow(advertising))[-i]))
# fit the model using this train control object
mKNNvs = train(FE~., method = "knn", data = cars2010,
    trControl = tc)
@ 
  \item Using the same validation set, estimate the performance of the k nearest neighbours algorithm for different values of $k$.
<<>>=
mKNNvs2 = train(FE~., method = "knn", data = cars2010,
     trControl = tc, tuneGrid = data.frame(k= 2:20))
@ 
  \item Which model is chosen as the best when using the validation set approach? -- With the random seed set as 1 at the top the best model here is:
<<>>=
mKNNvs2$bestTune
@
  \item Create new \cc{trainControl} objects to specify the use of 5 fold and 10 fold cross validation as well as bootstrapping to estimate test MSE.
<<>>=
tc5fold = trainControl(method = "cv", number = 5)

tc10fold = trainControl(method = "cv", number = 10)

tcboot = trainControl(method = "boot",
    number = 50)# use 50 boot strap estimates
@ 
  \item Go through the same training procedure attempting to find the best KNN model.
<<>>=
mKNNcv5 = train(FE~., data = cars2010, method = "knn",
    trControl = tc5fold, tuneGrid = data.frame(k = 2:20))

mKNNcv10 = train(FE~., data = cars2010, method = "knn",
    trControl = tc10fold, tuneGrid = data.frame(k = 2:20))

mKNNboot = train(FE~., data = cars2010, method = "knn",
    trControl = tcboot, tuneGrid = data.frame(k = 2:20))

mKNNcv5$bestTune

mKNNcv10$bestTune

mKNNboot$bestTune
@ 
  \item How do the results vary based on the method of estimation? -- The k-fold cross validation estimates and bootstrap estimates all yield the same conclusion, however it is different to when we used validation set approach earlier. We could plot the results from each on one plot to compare further:
<<fig.height = 5, fig.width = 5>>=
plot(2:20, mKNNboot$results[,2],type = "l", ylab = "RMSE",
     xlab = "k", ylim = c(3,6.5))
lines(2:20, mKNNcv10$results[,2],col = "red")
lines(2:20, mKNNcv5$results[,2], col = "blue")
lines(2:20, mKNNvs2$results[,2],col = "green")
@ 
  \item Are the conclusions always the same? -- no see previous answer
  \item Which method is the most computationally efficient?
<<>>=
mKNNvs2$time$everything
mKNNcv5$time$everything
mKNNcv10$time$everything
mKNNboot$time$everything
@ 
The validation set approach was quickest, however we must bear in mind that the conclusion here was different to the other cross validation approaches. The two k--fold cross validation estimates of RMSE and the bootstrap estimates all agreed with each other lending more weight to their conclusions. Plus we saw in the lectures that validation set approach was prone to highly variable estimates meaning we could get a different conclusion using a different hold out set.
Either of the two k--fold cross validation methods would be preferable here.
\item Try fitting a lasso, ridge and elastic net model using all of the main effects, pairwise interactions and square terms from each of the predictors.
<<>>=
## load the data in 
data(diabetes, package = "lars")

diabetesdata = cbind(diabetes$x,"y" = diabetes$y)

modelformula = as.formula(paste("y~(.)^2 + ",
    paste("I(",colnames(diabetesdata),"^2)", 
          collapse = "+", sep = "")
    ))
mLASSO = train(modelformula, data = diabetesdata,
    method = "lasso")
mRIDGE = train(modelformula, data = diabetesdata,
    method = "ridge")
mENET = train(modelformula, data = diabetesdata,
    method = "enet")
@ 
  \item Try to narrow in on the region of lowest RMSE for each model, don't forget about the \cc{tuneGrid} argument to the train function.
<<>>=
# examine previous output then train over a finer grid near the better end
mLASSOfine = train(modelformula,data = diabetesdata,
    method = "lasso", tuneGrid = data.frame(fraction = seq(0.1,0.5,by = 0.05)))
mLASSOfine$results
# best still right down at the 0.1 end
mLASSOfiner = train(modelformula,data = diabetesdata,
    method = "lasso", tuneGrid = data.frame(fraction = seq(0.01,0.15,by = 0.01)))
mLASSOfiner$results
# 0.09 seems the best

mRIDGEfine = train(modelformula,data = diabetesdata,
    method = "ridge", tuneGrid = data.frame(lambda = seq(0,0.1,by = 0.01)))
mRIDGEfine$results
mRIDGEfiner = train(modelformula,data = diabetesdata,
    method = "ridge", tuneGrid = data.frame(lambda = seq(0.005,0.03,by = 0.001)))
mRIDGEfiner$results
# 0.023 seems best

mENETfine = train(modelformula, data = diabetesdata, 
    method = "enet", tuneGrid = expand.grid(
                         lambda = c(0.001,0.01,0.1),
                         fraction = c(0.4,0.5,0.6)
    ))
mENETfine$results

mENETfiner = train(modelformula, data = diabetesdata, 
    method = "enet", tuneGrid = expand.grid(
                         lambda = seq(0.001,0.1,length.out = 10),
                         fraction = 0.5))
mENETfiner$results
# 0.012, 0.5 best
@ 

  \item How many features have been chosen by the lasso and enet models?
<<>>=
# use predict to find the coefficients
coefLASSO = predict(mLASSOfiner$finalModel, mode = "fraction", 
        type = "coefficient", s = 0.09
        )
sum(coefLASSO$coefficients != 0)
coefENET= predict(mENETfiner$finalModel, mode = "fraction", 
        type = "coefficient", s = 0.5
        )
sum(coefENET$coefficients != 0)
@ 
  \item How do these models compare to principal components and partial least squares regression?
<<>>=
mPCR = train(modelformula, data = diabetesdata, method = "pcr",
             tuneGrid = data.frame(ncomp = 1:7))
mPLS = train(modelformula, data = diabetesdata, method = "pls",
             tuneGrid = data.frame(ncomp= 1:7))
mPLS2 = train(modelformula, data = diabetesdata, method = "pls",
             tuneGrid = data.frame(ncomp= 5:15))
getTrainPerf(mLASSOfiner)
getTrainPerf(mRIDGEfiner)
getTrainPerf(mENETfiner)
getTrainPerf(mPCR)
getTrainPerf(mPLS2)
@
The elastic net model has the lowest estimated test error, all are fairly similar. The elastic net model suggests only 21 non--zero coefficients out of all of those included in the model.
  \item Have a go at writing a function that will allow a regression model to be chosen by the absolute value of the largest residual and try using it to fit a couple of models.
<<>>=
maxabsres = function(data, lev = NULL, model = NULL){
    m = max(abs(data$obs - data$pred))
    return(c("Max" = m))
}
# test with pls regression
tccustom = trainControl(method = "cv",
                     summaryFunction = maxabsres)
mPLScustom = train(FE~., data = cars2010, method = "pls",
               tuneGrid = data.frame(ncomp = 1:6),
               trControl = tccustom,
               metric = "Max", maximize = FALSE)
# success
# not to sugges this is a good choice of metric
@ 
\end{itemize}
\end{document}
